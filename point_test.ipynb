{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e5baf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载 14218 个顶点。\n",
      "数据形状: (14218, 4)\n",
      "\n",
      "前几个顶点的数据:\n",
      "[[0.025637 1.416699 0.11155  1.      ]\n",
      " [0.021846 1.418134 0.112966 1.      ]\n",
      " [0.023548 1.422539 0.112968 1.      ]\n",
      " [0.026068 1.425363 0.112705 1.      ]\n",
      " [0.029768 1.41646  0.110247 1.      ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_vertices(filepath):\n",
    "    \"\"\"\n",
    "    从指定的文本文件中加载顶点数据。\n",
    "    数据格式要求：每行包含以空格分隔的浮点数（例如：x y z w）。\n",
    "\n",
    "    参数:\n",
    "        filepath (str): 文件的完整路径。\n",
    "\n",
    "    返回:\n",
    "        numpy.ndarray: 包含顶点数据的二维 NumPy 数组。\n",
    "                       例如，形状为 (N, 4)，其中 N 是顶点的数量。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # numpy.loadtxt 非常适合读取以空格（或其他分隔符）分隔的数值数据\n",
    "        # 默认 dtype 是 float，默认 delimiter 是任何空白字符，正好符合您的需求\n",
    "        vertices = np.loadtxt(filepath, dtype=np.float64)\n",
    "        \n",
    "        # 简单检查数据的形状\n",
    "        if vertices.ndim != 2 or vertices.shape[1] != 4:\n",
    "            print(f\"警告: 读取的数据形状为 {vertices.shape}，可能不是预期的 (N, 4) 格式。\")\n",
    "        \n",
    "        return vertices\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 文件未找到 - {filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时发生错误: {e}\")\n",
    "        return None\n",
    "\n",
    "# 定义文件路径\n",
    "file_path = \"C:\\\\Users\\\\31878\\\\Desktop\\\\standardVertex.txt\"\n",
    "\n",
    "# 调用函数加载顶点数据\n",
    "vertex_data = load_vertices(file_path)\n",
    "\n",
    "# 返回结果\n",
    "if vertex_data is not None:\n",
    "    print(f\"成功加载 {vertex_data.shape[0]} 个顶点。\")\n",
    "    print(f\"数据形状: {vertex_data.shape}\")\n",
    "    print(\"\\n前几个顶点的数据:\")\n",
    "    print(vertex_data[:5]) # 打印前5行数据进行验证\n",
    "\n",
    "# vertex_data 现在就是一个 NumPy 数组，包含了文件中的所有顶点数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b38eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 77, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 默认的BN动量 (在TensorFlow中通常是衰减率，PyTorch中是动量，两者关系为 momentum = 1 - decay)\n",
    "# PointNet的原始实现通常使用 0.9 或 0.99，所以这里使用 0.1 作为动量 (1 - 0.9)\n",
    "DEFAULT_BN_MOMENTUM = 0.1 \n",
    "\n",
    "class TokenGenerator(nn.Module):\n",
    "    def __init__(self, in_dim=1024, num_tokens=77, token_dim=768):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_dim, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, token_dim)\n",
    "            ) for _ in range(num_tokens)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1024)\n",
    "        tokens = [mlp(x) for mlp in self.token_embeddings]  # list of (B, 768)\n",
    "        return torch.stack(tokens, dim=1)  # (B, 77, 768)\n",
    "\n",
    "\n",
    "class TNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Transform Net for PointNet. \n",
    "    Can be used for both input (K=3) and feature (K=64) transformation.\n",
    "    \"\"\"\n",
    "    def __init__(self, K=3, bn_decay=None):\n",
    "        super(TNet, self).__init__()\n",
    "        self.K = K\n",
    "        # 使用提供的bn_decay或默认值\n",
    "        momentum = bn_decay if bn_decay is not None else DEFAULT_BN_MOMENTUM\n",
    "\n",
    "        # 卷积层: Conv2D [1, K] (K=3 或 64), 卷积核大小 (1, K)\n",
    "        # 输入维度: (B, C_in, N, F) -> (B, 1, N, K) for Conv2d\n",
    "        self.conv1 = nn.Sequential(\n",
    "            # [1, 3] 或 [1, 64] 卷积核, 步长 [1, 1], padding=(0, 0)\n",
    "            # 对于 Conv2d，输入是 (B, 1, N, K)，kernel_size=(1, K) 意味着它覆盖整个特征维度 K。\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(1, K), stride=(1, 1), padding=(0, 0), bias=False),\n",
    "            nn.BatchNorm2d(64, momentum=momentum),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 1x1 卷积 (对于 N 个点独立)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), bias=False),\n",
    "            nn.BatchNorm2d(128, momentum=momentum),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), bias=False),\n",
    "            nn.BatchNorm2d(1024, momentum=momentum),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # 全连接层 (应用于最大池化后的特征)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(1024, 512, bias=False),\n",
    "            nn.BatchNorm1d(512, momentum=momentum),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(512, 256, bias=False),\n",
    "            nn.BatchNorm1d(256, momentum=momentum),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # 变换矩阵预测层: 256 -> K*K\n",
    "        self.transform_fc = nn.Linear(256, K * K)\n",
    "\n",
    "        # 初始化权重和偏置以确保初始变换矩阵接近单位矩阵 (Identity)\n",
    "        nn.init.constant_(self.transform_fc.weight, 0)\n",
    "        \n",
    "        # 偏置初始化为单位矩阵的展平形式\n",
    "        identity = torch.eye(K).flatten()\n",
    "        # 确保偏置维度匹配\n",
    "        if self.transform_fc.bias is not None and self.transform_fc.bias.numel() == K * K:\n",
    "             self.transform_fc.bias.data.copy_(identity)\n",
    "        else: # 如果 bias 是 False 或者维度不匹配，则忽略偏置初始化\n",
    "            pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, K) or (B, N, 3) \n",
    "        batch_size = x.size(0)\n",
    "        num_point = x.size(1)\n",
    "        K = self.K\n",
    "\n",
    "        # 1. 适配 Conv2d: (B, N, K) -> (B, 1, N, K)\n",
    "        # tf.expand_dims(point_cloud, -1) 得到 (B, N, K, 1)，然后 Conv2D [1, K]\n",
    "        # 在 PyTorch 中，我们使用 (B, C_in, H, W) = (B, 1, N, K)\n",
    "        # 我们将特征维度 K 视为 W，N 视为 H，并添加 C_in=1 通道。\n",
    "        net = x.unsqueeze(1) # (B, N, K) -> (B, 1, N, K)\n",
    "\n",
    "        # 2. 卷积层\n",
    "        net = self.conv1(net) \n",
    "        # K=3: (B, 1, N, 3) -> Conv(1x3) -> (B, 64, N, 1)\n",
    "        # K=64: (B, 1, N, 64) -> Conv(1x64) -> (B, 64, N, 1)\n",
    "        net = self.conv2(net) # (B, 128, N, 1)\n",
    "        net = self.conv3(net) # (B, 1024, N, 1)\n",
    "\n",
    "        # 3. Max Pool: [num_point, 1]\n",
    "        net = F.max_pool2d(net, kernel_size=(num_point, 1)) # (B, 1024, 1, 1)\n",
    "\n",
    "        # 4. Reshape\n",
    "        net = net.view(batch_size, -1) # (B, 1024)\n",
    "\n",
    "        # 5. 全连接层\n",
    "        net = self.fc1(net) # (B, 512)\n",
    "        net = self.fc2(net) # (B, 256)\n",
    "\n",
    "        # 6. 变换矩阵\n",
    "        transform = self.transform_fc(net) # (B, K*K)\n",
    "        transform = transform.view(batch_size, K, K) # (B, K, K)\n",
    "\n",
    "        return transform\n",
    "\n",
    "# 假设 PointNetCls 是主模型\n",
    "class PointNetCls(nn.Module):\n",
    "    \"\"\" Classification PointNet, input is BxNx3, output Bx40 \"\"\"\n",
    "    def __init__(self, num_classes=40, bn_decay=None,in_dim=1024, num_tokens=77, token_dim=768):\n",
    "        super(PointNetCls, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # 使用提供的bn_decay或默认值\n",
    "        momentum = bn_decay if bn_decay is not None else DEFAULT_BN_MOMENTUM\n",
    "        \n",
    "        # T-Nets\n",
    "        self.input_transform_net = TNet(K=3, bn_decay=momentum)\n",
    "        self.feature_transform_net = TNet(K=64, bn_decay=momentum) # K=64 for feature space\n",
    "\n",
    "        # Shared MLP (Convolutional Layers)\n",
    "        # 1. 64 out\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 0), bias=False),\n",
    "            nn.BatchNorm2d(64, momentum=momentum),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 2. 64 out (1x1)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), bias=False),\n",
    "            nn.BatchNorm2d(64, momentum=momentum),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 3. 64 out (1x1) - After Feature T-Net\n",
    "        self.conv3 = nn.Sequential(\n",
    "            # 注意: 尽管特征变换后维度仍是 64，但这里使用 nn.Conv1d \n",
    "            # 因为经过特征变换后的 net_transformed 是 (B, N, 64) -> (B, 64, N) \n",
    "            # 原始代码是 Conv2D [1, 1] on (B, N, 1, 64)\n",
    "            # 我们将使用 Conv1d on (B, 64, N)\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm1d(64, momentum=momentum),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # 4. 128 out (1x1)\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm1d(128, momentum=momentum),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 5. 1024 out (1x1) - Global Feature Vector\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=1024, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm1d(1024, momentum=momentum),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Classification MLP\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(1024, 512, bias=False),\n",
    "            nn.BatchNorm1d(512, momentum=momentum),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(p=0.3) # keep_prob=0.7 -> dropout_prob=0.3\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(512, 256, bias=False),\n",
    "            nn.BatchNorm1d(256, momentum=momentum),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(p=0.3) # keep_prob=0.7 -> dropout_prob=0.3\n",
    "\n",
    "        self.fc3 = nn.Linear(256, num_classes) # No activation_fn\n",
    "\n",
    "        self.tokenGenerator=TokenGenerator(in_dim=in_dim, num_tokens=num_tokens, token_dim=token_dim)\n",
    "\n",
    "\n",
    "    def forward(self, point_cloud):\n",
    "        # point_cloud: (B, N, 3)\n",
    "        batch_size = point_cloud.size(0)\n",
    "        num_point = point_cloud.size(1)\n",
    "        \n",
    "        # ------------------- Input T-Net -------------------\n",
    "        # 1. Input Transformation: BxNx3 -> Bx3x3\n",
    "        transform = self.input_transform_net(point_cloud) # (B, 3, 3)\n",
    "\n",
    "        # 2. Apply Transformation\n",
    "        # point_cloud (B, N, 3) x transform (B, 3, 3) -> (B, N, 3)\n",
    "        point_cloud_transformed = torch.bmm(point_cloud, transform) # BxNx3\n",
    "\n",
    "        # 3. Reshape for Conv2D: (B, N, 3) -> (B, 1, N, 3)\n",
    "        input_image = point_cloud_transformed.unsqueeze(1) # (B, 1, N, 3)\n",
    "\n",
    "        # 4. Shared MLP: Conv2D [1, 3] then Conv2D [1, 1]\n",
    "        # (B, 1, N, 3) -> (B, 64, N, 1)\n",
    "        net = self.conv1(input_image) \n",
    "        # (B, 64, N, 1) -> (B, 64, N, 1)\n",
    "        net = self.conv2(net)\n",
    "        \n",
    "        # 5. Reshape for Feature T-Net and next Conv1D: (B, 64, N)\n",
    "        net_feature = net.squeeze(-1) # (B, 64, N)\n",
    "        # Note: Original TF code works on (B, N, 1, 64), here we have (B, 64, N)\n",
    "        # We need to reshape to (B, N, 64) for matmul with T-Net output (B, 64, 64)\n",
    "        net_to_transform = net_feature.transpose(2, 1) # (B, 64, N) -> (B, N, 64)\n",
    "        \n",
    "        # ------------------- Feature T-Net -------------------\n",
    "        # 6. Feature Transformation: BxNx64 -> Bx64x64\n",
    "        feature_transform = self.feature_transform_net(net_to_transform) # (B, 64, 64)\n",
    "\n",
    "        # 7. Apply Transformation\n",
    "        # net_to_transform (B, N, 64) x feature_transform (B, 64, 64) -> (B, N, 64)\n",
    "        net_transformed = torch.bmm(net_to_transform, feature_transform) # BxNx64\n",
    "\n",
    "        # 8. Reshape for next Conv1D (B, N, 64) -> (B, 64, N)\n",
    "        net = net_transformed.transpose(2, 1) # (B, 64, N)\n",
    "\n",
    "        # 9. Shared MLP: Conv1D [1]\n",
    "        net = self.conv3(net) # (B, 64, N) -> (B, 64, N)\n",
    "        net = self.conv4(net) # (B, 64, N) -> (B, 128, N)\n",
    "        net = self.conv5(net) # (B, 128, N) -> (B, 1024, N)\n",
    "\n",
    "        # 10. Symmetric Function: Max Pooling (Global Feature)\n",
    "        # tf_util.max_pool2d(net, [num_point,1]) \n",
    "        # In PyTorch, we pool over the N dimension (dim=2)\n",
    "        global_feature = F.max_pool1d(net, kernel_size=num_point) # (B, 1024, 1)\n",
    "        \n",
    "        # 11. Reshape\n",
    "        net = global_feature.view(batch_size, -1) # (B, 1024)\n",
    "\n",
    "        net = self.tokenGenerator(net)\n",
    "\n",
    "        # 12. Classification MLP\n",
    "        # net = self.fc1(net) # (B, 512)\n",
    "        # net = self.dropout1(net)\n",
    "        \n",
    "        # net = self.fc2(net) # (B, 256)\n",
    "        # net = self.dropout2(net)\n",
    "        \n",
    "        # logits = self.fc3(net) # (B, 40)\n",
    "\n",
    "        # 13. Return logits and end_points (transformation matrix)\n",
    "        # end_points = {'transform': feature_transform}\n",
    "\n",
    "        return net\n",
    "        \n",
    "        # return logits, end_points\n",
    "\n",
    "# 示例调用\n",
    "point_cloud_data = torch.randn(32, 1024, 3) # (B, N, 3)\n",
    "model = PointNetCls(num_classes=40)\n",
    "output = model(point_cloud_data)\n",
    "print(output.shape) # torch.Size([32, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c54abf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n",
      "torch.Size([2, 77, 1536])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 137\u001b[0m\n\u001b[0;32m    135\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m    136\u001b[0m         opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 137\u001b[0m         importance, out\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m#     loss, recon_loss, bal_loss = loss_fn(x, out, importance)\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m#     loss.backward()\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m#     opt.step()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m \n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# ==================== 推理示例 =====================\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\31878\\miniconda3\\envs\\combine\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\31878\\miniconda3\\envs\\combine\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[6], line 97\u001b[0m, in \u001b[0;36mMoEEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     94\u001b[0m input_i \u001b[38;5;241m=\u001b[39m flat[i] \u001b[38;5;66;03m# 当前的输入向量 [D]\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k):\n\u001b[1;32m---> 97\u001b[0m     expert_id \u001b[38;5;241m=\u001b[39m \u001b[43mtop_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m     weight \u001b[38;5;241m=\u001b[39m top_val[i, k]\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# 计算当前专家对当前输入的输出\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ====================== 依赖 ======================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# ====================== 超参 ======================\n",
    "BATCH_SIZE      = 2\n",
    "SEQ_LEN         = 77\n",
    "FEAT_DIM        = 768\n",
    "NUM_EXPERTS     = 2\n",
    "TOP_K           = 2\n",
    "HIDDEN_DIM      = 512\n",
    "OUT_DIM         = 768\n",
    "EPOCHS          = 5\n",
    "LR              = 1e-3\n",
    "LAMBDA_BALANCE  = 0.01\n",
    "DEVICE          = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ==================== 随机数据 =====================\n",
    "# 制造 1000 条伪序列，仅用于演示\n",
    "X = np.random.randn(1000, SEQ_LEN, FEAT_DIM).astype(np.float32)\n",
    "train_ds = TensorDataset(torch.from_numpy(X))\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ==================== 模型 =========================\n",
    "class MoEEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim=FEAT_DIM,\n",
    "                 hidden_dim=HIDDEN_DIM,\n",
    "                 output_dim=OUT_DIM,\n",
    "                 num_experts=NUM_EXPERTS,\n",
    "                 top_k=TOP_K,\n",
    "                 point_cloud_data = torch.randn(BATCH_SIZE, 1024, 3)):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "\n",
    "        # 门控\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "\n",
    "        # 专家网络\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, output_dim)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        self.point_net = PointNetCls(num_classes=40)\n",
    "\n",
    "        self.point_cloud_data = point_cloud_data\n",
    "        # self.register_buffer('point_cloud_data', point_cloud_data)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, T, D]  ->  out: [B, T, D]\n",
    "        同时返回负载均衡统计量\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        flat = x.reshape(B * T, D)                      # [B*T, D]\n",
    "        # print(flat.shape)\n",
    "\n",
    "        gate_logits = self.gate(flat)                   # [B*T, num_experts]\n",
    "        # print(gate_logits.shape)\n",
    "        # print(gate_logits[:5])\n",
    "\n",
    "        gate_probs = F.softmax(gate_logits, dim=-1)     # [B*T, num_experts]\n",
    "        top_val, top_idx = torch.topk(gate_probs, TOP_K, dim=-1)  # [B*T, top_k]\n",
    "        # print(top_val.shape)\n",
    "        # print(top_val[:5])\n",
    "        # print(top_idx[:5])\n",
    "\n",
    "        # 累加每个专家被选中权重的和，用于负载均衡损失\n",
    "        importance = torch.zeros(self.num_experts, device=x.device)\n",
    "        importance.index_add_(0, top_idx.view(-1), top_val.view(-1))\n",
    "\n",
    "        # 加权求和专家输出\n",
    "        out = torch.zeros_like(flat)\n",
    "\n",
    "        # 初始化用于分别记录 expert 1 和 expert 2 输出的累加器\n",
    "        # 它们的形状应该是 [D]，即单个样本的输出维度\n",
    "        # 我们使用 flat.size(-1) 来获取输出维度 D\n",
    "        D = flat.size(-1)\n",
    "        expert_1_accumulated_output = torch.zeros_like(flat)\n",
    "        expert_2_accumulated_output = torch.zeros_like(flat)\n",
    "\n",
    "        pointfeature = self.point_net(self.point_cloud_data.to(x.device))\n",
    "\n",
    "        for i in range(B * T):\n",
    "            input_i = flat[i] # 当前的输入向量 [D]\n",
    "\n",
    "            for k in range(self.top_k):\n",
    "                expert_id = top_idx[i, k].item()\n",
    "                weight = top_val[i, k]\n",
    "                \n",
    "                # 计算当前专家对当前输入的输出\n",
    "                expert_output = self.experts[expert_id](input_i) # [D]\n",
    "\n",
    "                # 累加到最终的 out\n",
    "                out[i] += weight * expert_output\n",
    "\n",
    "\n",
    "                if expert_id == 0:\n",
    "                    expert_1_accumulated_output[i] += weight * expert_output\n",
    "                elif expert_id == 1:\n",
    "                    expert_2_accumulated_output[i] += weight * expert_output\n",
    "                # -------------------------------------\n",
    "\n",
    "        final_point = expert_1_accumulated_output.view(B, T, -1) + pointfeature\n",
    "        final_out=torch.cat([final_point, expert_2_accumulated_output.view(B, T, -1)], dim=2)\n",
    "\n",
    "\n",
    "        return importance, final_out\n",
    "\n",
    "\n",
    "# ==================== 损失 =========================\n",
    "def loss_fn(x, recon, importance, lambda_bal=LAMBDA_BALANCE):\n",
    "    recon_loss = F.mse_loss(recon, x)                   # 重构\n",
    "    balance_loss = torch.std(importance)                # 专家使用均衡程度\n",
    "    return recon_loss + lambda_bal * balance_loss, recon_loss, balance_loss\n",
    "\n",
    "\n",
    "# ==================== 训练 =========================\n",
    "model = MoEEncoder().to(DEVICE)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss, total_recon, total_bal = 0., 0., 0.\n",
    "    for (x,) in train_loader:\n",
    "        x = x.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        importance, out= model(x)\n",
    "        print(out.shape)\n",
    "    #     loss, recon_loss, bal_loss = loss_fn(x, out, importance)\n",
    "    #     loss.backward()\n",
    "    #     opt.step()\n",
    "\n",
    "    #     total_loss  += loss.item()\n",
    "    #     total_recon += recon_loss.item()\n",
    "    #     total_bal   += bal_loss.item()\n",
    "\n",
    "    # print(f\"Epoch {epoch:02d} | total {total_loss/len(train_loader):.4f} | \"\n",
    "    #       f\"recon {total_recon/len(train_loader):.4f} | balance {total_bal/len(train_loader):.4f}\")\n",
    "\n",
    "# ==================== 推理示例 =====================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    demo = torch.randn(5, SEQ_LEN, FEAT_DIM).to(DEVICE)\n",
    "    feat, _ = model(demo)          # [5, 77, 768]\n",
    "    print(\"输出形状:\", feat.shape)  # 应输出 torch.Size([5, 77, 768])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "combine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
