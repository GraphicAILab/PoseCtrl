{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 把底模safetensors转化为文件夹权重模式\n",
    "\n",
    "在这里下载底模https://civitai.com/models/27259?modelVersionId=221220然后移动到文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "!wget --content-disposition \"https://civitai.com/api/download/models/221220?type=Model&format=SafeTensor&size=pruned&fp=fp16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "checkpoint_path = r\"tmndMix_tmndMixSPRAINBOW.safetensors\"\n",
    "save_path = r\"basemodel\"\n",
    "# 加载 .safetensors 文件\n",
    "pipeline = StableDiffusionPipeline.from_single_file(checkpoint_path)\n",
    "\n",
    "# 将模型保存为 diffusers 格式\n",
    "pipeline.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 把数据集解压\n",
    "\n",
    "数据集要的所有东西全部放在共享文件夹了（images）  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip images/image_resized.zip\n",
    "!cp images/merged_joints2d.txt image_resized/merged_joints2d.txt\n",
    "!cp images/filtered_camera_params.txt image_resized/camera_params.txt\n",
    "!cp images/image_features.txt image_resized/image_features.txt\n",
    "!cp images/image_smpl_normal.txt image_resized/image_smpl.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip images/image_mirror_resized.zip\n",
    "!cp images/mirror_merged_joints2d.txt image_resized/merged_joints2d.txt\n",
    "!cp images/mirror_filtered_camera_params.txt image_mirror_resized/camera_params.txt\n",
    "!cp images/image_features.txt image_mirror_resized/image_features.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/sodabreak/PoseCtrl.git\n",
    "!cd poseCtrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单卡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train_colab_V7_AllData.py --pretrained_model_name_or_path \"/basemodel\" --base_point_path \"/PoseCtrl/dataSet/standardVertex_2.txt\" --data_root_path_2 \"/image_resized\" --data_root_path_3 \"/image_mirror_resized\" --train_batch_size 4 --save_steps 2000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多卡\n",
    "\n",
    "- 先运行配置\n",
    "- 回答问题：使用混合精度fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate launch --num_processes 2 --multi_gpu --mixed_precision \"fp16\" train_colab_V6_AllData.py --pretrained_model_name_or_path \"/basemodel\" --base_point_path \"/PoseCtrl/dataSet/standardVertex_2.txt\" --data_root_path_2 \"/image_resized\" --data_root_path_3 \"/image_mirror_resized\" --train_batch_size 4 --save_steps 2000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 权重处理\n",
    "\n",
    "所有训练得到的权重都在/sd-pose_ctrl文件夹内\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" V6 \"\"\"\n",
    "import torch\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "current_dir = 'content'\n",
    "\n",
    "sys.path.append('/content/PoseCtrl')\n",
    "sys.path.append('/content/PoseCtrl/poseCtrl')\n",
    "from poseCtrl.models.pose_adaptor import VPmatrixPoints, ImageProjModel\n",
    "from poseCtrl.models.attention_processor import AttnProcessor, PoseAttnProcessor\n",
    "from poseCtrl.data.dataset import CombinedDataset, load_base_points\n",
    "from poseCtrl.models.posectrl import PoseCtrl,PoseCtrlV1,PoseCtrlV5\n",
    "import numpy as np\n",
    "from poseCtrl.pipelines.Pose_pipelines import PoseControlNet\n",
    "from poseCtrl.pipelines.Pose_pipelines import PoseControlNet, PoseControlNetV6\n",
    "\n",
    "base_point_path=r'/content/drive/MyDrive/PoseCtrl/dataSet/standardVertex_2.txt'\n",
    "raw_base_points=load_base_points(base_point_path)  \n",
    "base_model_path = r\"/content/drive/MyDrive/basemodel\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "ip_ckpt = r\"/content/drive/MyDrive/ckpt/points_posectrl2000.bin\"\n",
    "device = \"cuda\"\n",
    "def image_grid(imgs_np: np.ndarray, rows: int, cols: int):\n",
    "    \"\"\"\n",
    "    imgs_np: np.ndarray of shape (B, H, W, C) and values in [0, 1] (float)\n",
    "    rows, cols: number of rows and columns in grid\n",
    "    \"\"\"\n",
    "    assert imgs_np.ndim == 4 and imgs_np.shape[0] == rows * cols, \"Input shape must be (B, H, W, C)\"\n",
    "    B, H, W, C = imgs_np.shape\n",
    "\n",
    "    # 归一化 -> uint8 -> PIL\n",
    "    pil_imgs = [Image.fromarray((img * 255).astype(np.uint8)) for img in imgs_np]\n",
    "\n",
    "    # 创建大图\n",
    "    grid_img = Image.new('RGB', size=(cols * W, rows * H))\n",
    "    \n",
    "    for idx, img in enumerate(pil_imgs):\n",
    "        x = (idx % cols) * W\n",
    "        y = (idx // cols) * H\n",
    "        grid_img.paste(img, box=(x, y))\n",
    "\n",
    "    return grid_img\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "\n",
    "# load SD pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "val_dataset = CombinedDataset(\n",
    "        # path1=args.data_root_path_1,\n",
    "        path2=\"/content/drive/MyDrive/images_01/test\",\n",
    "        tokenizer=pipe.tokenizer,\n",
    "    )\n",
    "\n",
    "\n",
    "data = val_dataset[0]\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Resize((256, 256))\n",
    "\n",
    "\"\"\" denormalize 是因为dataloader的图片加载完之后会被transform，正常的图片只要是PIL就行 \"\"\"\n",
    "def denormalize(tensor, mean, std):\n",
    "    return tensor * std + mean\n",
    "\n",
    "mean = 0.5\n",
    "std = 0.5\n",
    "\n",
    "\n",
    "image = data['image']\n",
    "points = data['joints_image']\n",
    "image = denormalize(image, mean, std)\n",
    "image_pil = transforms.ToPILImage()(image) \n",
    "points_pil = transforms.ToPILImage()(points)\n",
    "points = data['joints_image'].to(torch.float16).unsqueeze(0).to(device)\n",
    "vmatrix = data['view_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "pmatrix = data['projection_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "text = 'highly detailed, anime, 1girl, blue_eyes, purple long_hair, black dress, smile, white simple background'\n",
    "pose_model = PoseControlNetV6(pipe, image_encoder_path, ip_ckpt, raw_base_points, device,8)\n",
    "images = pose_model.generate( prompt = text, num_samples=4, num_inference_steps=50, seed=42, V_matrix=vmatrix, P_matrix=pmatrix, points=points)\n",
    "grid = image_grid(images, 1, 4)\n",
    "combined_image = Image.new('RGB', (6*image_pil.width, image_pil.height))\n",
    "combined_image.paste(image_pil, (0, 0))\n",
    "combined_image.paste(points_pil, (image_pil.width, 0))\n",
    "combined_image.paste(grid, (2*image_pil.width, 0))\n",
    "combined_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" V7 \"\"\"\n",
    "import torch\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "current_dir = r'F:\\Projects\\diffusers\\Project'\n",
    "current_dir = os.path.join(current_dir, \"PoseCtrl\")\n",
    "sys.path.append(current_dir)\n",
    "sys.path.append(os.path.join(current_dir,\"poseCtrl\"))\n",
    "from poseCtrl.models.pose_adaptor import VPmatrixPoints, ImageProjModel\n",
    "from poseCtrl.models.attention_processor import AttnProcessor, PoseAttnProcessor\n",
    "from poseCtrl.data.dataset import CombinedDataset, load_base_points, CombinedDatasetTest\n",
    "from poseCtrl.models.posectrl import PoseCtrl,PoseCtrlV1,PoseCtrlV5\n",
    "import numpy as np\n",
    "from poseCtrl.pipelines.Pose_pipelines import PoseControlNet\n",
    "from poseCtrl.pipelines.Pose_pipelines import PoseControlNet, PoseControlNetV6, PoseControlNetV7\n",
    "base_point_path=r\"F:\\Projects\\diffusers\\Project\\PoseCtrl\\dataSet\\standardVertex_2.txt\"\n",
    "raw_base_points=load_base_points(base_point_path)  \n",
    "\n",
    "base_model_path = r\"F:\\Projects\\diffusers\\ProgramData\\basemodel\"\n",
    "# vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = r\"C:\\Users\\30631\\.cache\\huggingface\\hub\\models--laion--CLIP-ViT-H-14-laion2B-s32B-b79K\\snapshots\\1c2b8495b28150b8a4922ee1c8edee224c284c0c\"\n",
    "ip_ckpt = r\"F:\\Projects\\diffusers\\Backup\\ckpt\\posectrl.bin\"\n",
    "device = \"cuda\"\n",
    "def image_grid(imgs_np: np.ndarray, rows: int, cols: int):\n",
    "    \"\"\"\n",
    "    imgs_np: np.ndarray of shape (B, H, W, C) and values in [0, 1] (float)\n",
    "    rows, cols: number of rows and columns in grid\n",
    "    \"\"\"\n",
    "    assert imgs_np.ndim == 4 and imgs_np.shape[0] == rows * cols, \"Input shape must be (B, H, W, C)\"\n",
    "    B, H, W, C = imgs_np.shape\n",
    "\n",
    "    # 归一化 -> uint8 -> PIL\n",
    "    pil_imgs = [Image.fromarray((img * 255).astype(np.uint8)) for img in imgs_np]\n",
    "\n",
    "    # 创建大图\n",
    "    grid_img = Image.new('RGB', size=(cols * W, rows * H))\n",
    "    \n",
    "    for idx, img in enumerate(pil_imgs):\n",
    "        x = (idx % cols) * W\n",
    "        y = (idx // cols) * H\n",
    "        grid_img.paste(img, box=(x, y))\n",
    "\n",
    "    return grid_img\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "# vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "\n",
    "# load SD pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "\n",
    "val_dataset = CombinedDatasetTest(\n",
    "        # path1=args.data_root_path_1,\n",
    "        path2=r\"F:\\\\Projects\\\\diffusers\\\\ProgramData\\\\test\",\n",
    "        # path3=args.data_root_path_3,\n",
    "        # path4=args.data_root_path_4,\n",
    "        # path5=args.data_root_path_5,\n",
    "        tokenizer=pipe.tokenizer,\n",
    "        txt_subdir_name=\"F:\\\\Projects\\\\diffusers\\\\ProgramData\\\\new_data\\\\image\\\\smpl\"\n",
    "    )\n",
    "\n",
    "data = val_dataset[0]\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Resize((256, 256))\n",
    "\n",
    "\"\"\" denormalize 是因为dataloader的图片加载完之后会被transform，正常的图片只要是PIL就行 \"\"\"\n",
    "def denormalize(tensor, mean, std):\n",
    "    return tensor * std + mean\n",
    "\n",
    "mean = 0.5\n",
    "std = 0.5\n",
    "\n",
    "\n",
    "image = data['image']\n",
    "points = data['joints_image']\n",
    "image = denormalize(image, mean, std)\n",
    "image_pil = transforms.ToPILImage()(image) \n",
    "points_pil = transforms.ToPILImage()(points)\n",
    "# points = data['joints_image'].to(torch.float16).unsqueeze(0).to(device)\n",
    "points = data['points'].to(torch.float16).unsqueeze(0).to(device)\n",
    "vmatrix = data['view_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "pmatrix = data['projection_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "# text = 'highly detailed, anime, 1girl, blue_eyes, purple long_hair, black dress, smile, white simple background'\n",
    "text = data['text']\n",
    "\n",
    "pose_model = PoseControlNetV7(pipe, image_encoder_path, ip_ckpt, raw_base_points, device)\n",
    "\n",
    "images = pose_model.generate( prompt = text, num_samples=4, num_inference_steps=50, seed=42, V_matrix=vmatrix, P_matrix=pmatrix, points=points)\n",
    "grid = image_grid(images, 1, 4)\n",
    "combined_image = Image.new('RGB', (6*image_pil.width, image_pil.height))\n",
    "combined_image.paste(image_pil, (0, 0))\n",
    "combined_image.paste(points_pil, (image_pil.width, 0))\n",
    "combined_image.paste(grid, (2*image_pil.width, 0))\n",
    "combined_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 1. 把文件夹的selected_joints2d变成一个文件merged_joints2d\"\"\"\n",
    "import os\n",
    "\n",
    "def merge_joints2d(root_dir, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "        for folder_name in os.listdir(root_dir):\n",
    "            folder_path = os.path.join(root_dir, folder_name)\n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "\n",
    "            joints_path = os.path.join(folder_path, 'selected_joints2d.txt')\n",
    "            if os.path.exists(joints_path):\n",
    "                out_file.write(f\"# {folder_name}\\n\")  # 写入图像文件名\n",
    "                with open(joints_path, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        out_file.write(line.strip() + '\\n')\n",
    "                out_file.write('\\n')  # 每组之间加空行\n",
    "\n",
    "# 使用示例\n",
    "root_folder = r\"F:\\Projects\\diffusers\\ProgramData\\points\\output_2\\output\"\n",
    "output_txt = r\"F:\\Projects\\diffusers\\ProgramData\\points\\output_2\\output/merged_joints2d.txt\"\n",
    "merge_joints2d(root_folder, output_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  把param.py处理成smpl参数的一个文件image_smpl.txt \"\"\"\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "_VAR_RE = re.compile(r'^\\s*([^=\\s]+)\\s*=')\n",
    "\n",
    "\n",
    "def _extract_first_bracket_block(s: str) -> Tuple[str, int, int]:\n",
    "    \"\"\"\n",
    "    从字符串 s 中提取第一个以 '[' 开始、匹配到的 ']' 结束的片段。\n",
    "    返回 (片段字符串, start_idx, end_idx)；找不到或不匹配则抛错。\n",
    "    \"\"\"\n",
    "    start = s.find('[')\n",
    "    if start == -1:\n",
    "        raise ValueError(\"No '[' found.\")\n",
    "    depth = 0\n",
    "    for i in range(start, len(s)):\n",
    "        c = s[i]\n",
    "        if c == '[':\n",
    "            depth += 1\n",
    "        elif c == ']':\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                return s[start:i+1], start, i+1\n",
    "    raise ValueError(\"Unmatched '[' ']' in string.\")\n",
    "\n",
    "def _reshape_to_21x3(arr) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    接受 literal_eval 后的列表(可一维或二维)，返回严格 21x3 的二维 list[21][3]。\n",
    "    \"\"\"\n",
    "    # 若是二维：铺平\n",
    "    if isinstance(arr, list) and arr and isinstance(arr[0], (list, tuple)):\n",
    "        flat = [float(x) for row in arr for x in row]\n",
    "    elif isinstance(arr, list):\n",
    "        flat = [float(x) for x in arr]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported parsed type: {type(arr)}\")\n",
    "\n",
    "    if len(flat) != 63:\n",
    "        raise ValueError(f\"Total elements = {len(flat)}; expected 63 for 21x3.\")\n",
    "\n",
    "    return [flat[i:i+3] for i in range(0, 63, 3)]\n",
    "\n",
    "def tensors_to_block_txt(\n",
    "    input_path: str,\n",
    "    output_txt_path: str,\n",
    "    names_filter: Optional[List[str]] = None,\n",
    "    overwrite: bool = True\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    读取 input_path（如 synthesis_param.py），将每个赋值的 torch.tensor(...) 提取为 21x3，\n",
    "    写入同一个 output_txt_path 中，格式为：\n",
    "        VarName\n",
    "        a11 a12 a13\n",
    "        ...\n",
    "        a21 a22 a23\n",
    "\n",
    "        NextVarName\n",
    "        ...\n",
    "\n",
    "    参数：\n",
    "        input_path: 源文件路径（含多行 VarName=torch.tensor([...])）\n",
    "        output_txt_path: 输出 txt 文件路径\n",
    "        names_filter: 若提供，仅导出列表中的变量名\n",
    "        overwrite: True 则覆盖输出文件；False 则追加写（如果文件已存在）\n",
    "\n",
    "    返回：\n",
    "        统计 dict：{\"saved\": n, \"skipped\": m, \"errors\": [...], \"out_path\": \"...\", \"names\": [...]}\n",
    "    \"\"\"\n",
    "    mode = \"w\" if overwrite else \"a\"\n",
    "    saved, skipped = 0, 0\n",
    "    names_written: List[str] = []\n",
    "    errors: List[str] = []\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # 先把要写的块构造好，最后一次性写入（避免半成品）\n",
    "    blocks: List[str] = []\n",
    "\n",
    "    for ln, raw in enumerate(lines, start=1):\n",
    "        line = raw.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        m = _VAR_RE.match(line)\n",
    "        if not m:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        varname = m.group(1)\n",
    "        if names_filter is not None and varname not in names_filter:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rhs = line.split(\"=\", 1)[1]\n",
    "            bracket_block, _, _ = _extract_first_bracket_block(rhs)\n",
    "        except Exception as e:\n",
    "            errors.append(f\"Line {ln} ({varname}): cannot find bracket block: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            arr = ast.literal_eval(bracket_block)\n",
    "        except Exception as e:\n",
    "            errors.append(f\"Line {ln} ({varname}): literal_eval failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rows21x3 = _reshape_to_21x3(arr)\n",
    "        except Exception as e:\n",
    "            errors.append(f\"Line {ln} ({varname}): reshape failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 生成一个块文本：第一行是名称，后面 21 行三列，块间空行\n",
    "        block_lines = [varname] + [\" \".join(str(x) for x in row) for row in rows21x3]\n",
    "        blocks.append(\"\\n\".join(block_lines))\n",
    "        names_written.append(varname)\n",
    "        saved += 1\n",
    "\n",
    "    # 写出\n",
    "    if blocks:\n",
    "        os.makedirs(os.path.dirname(os.path.abspath(output_txt_path)) or \".\", exist_ok=True)\n",
    "        with open(output_txt_path, mode, encoding=\"utf-8\") as wf:\n",
    "            wf.write((\"\\n\\n\".join(blocks)).rstrip() + \"\\n\")  # 末尾补换行\n",
    "\n",
    "    return {\n",
    "        \"saved\": saved,\n",
    "        \"skipped\": skipped,\n",
    "        \"errors\": errors,\n",
    "        \"out_path\": os.path.abspath(output_txt_path),\n",
    "        \"names\": names_written,\n",
    "    }\n",
    "\n",
    "\n",
    "input=r\"F:\\Projects\\diffusers\\ProgramData\\new_data\\param.py\"\n",
    "out=r'F:\\Projects\\diffusers\\ProgramData\\new_data\\image_smpl.txt'\n",
    "# from your_module import tensors_to_block_txt\n",
    "stats = tensors_to_block_txt(\n",
    "    input_path=input,\n",
    "    output_txt_path=out,\n",
    "    names_filter=None,   # 只导出特定名字可传如 [\"Avatar_Carol_C1_HP_UI\"]\n",
    "    overwrite=True\n",
    ")\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  把output_vertex.txt处理成一个文件夹，含有每个图片的三维点信息\"\"\"\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# 行首 header：<name>.(png|jpg|jpeg):\n",
    "HEADER_RE = re.compile(r'^\\s*([^\\s:]+?\\.(?:png|jpg|jpeg))\\s*:', re.IGNORECASE)\n",
    "TRIPLE_RE = re.compile(\n",
    "    r'\\(\\s*([+-]?(?:\\d+(?:\\.\\d*)?|\\.\\d+)(?:[eE][+-]?\\d+)?)\\s*,\\s*'\n",
    "    r'([+-]?(?:\\d+(?:\\.\\d*)?|\\.\\d+)(?:[eE][+-]?\\d+)?)\\s*,\\s*'\n",
    "    r'([+-]?(?:\\d+(?:\\.\\d*)?|\\.\\d+)(?:[eE][+-]?\\d+)?)\\s*\\)'\n",
    ")\n",
    "\n",
    "def _flush_one(img_name: str, body: str, out_dir: Path):\n",
    "    if not img_name:\n",
    "        return\n",
    "    triples = TRIPLE_RE.findall(body)\n",
    "    if not triples:\n",
    "        return\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / f\"{Path(img_name).stem}.txt\"  # 文件名为去后缀\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{img_name}\\n\")  # 第一行保留完整图片名（含后缀）\n",
    "        for x, y, z in triples:\n",
    "            f.write(f\"{x} {y} {z}\\n\")\n",
    "    print(f\"[saved] {out_path}\")\n",
    "\n",
    "def split_streaming(input_path: str, out_dir: str = \"out_txts\"):\n",
    "    out_dir = Path(out_dir)\n",
    "    current_img = None\n",
    "    current_body_parts = []\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "        for line in fh:\n",
    "            m = HEADER_RE.search(line)\n",
    "            if m:\n",
    "                # 遇到新的 header：先把旧条目写出\n",
    "                if current_img is not None:\n",
    "                    _flush_one(current_img, \"\".join(current_body_parts), out_dir)\n",
    "                    current_body_parts = []\n",
    "\n",
    "                # 启动新条目，正文从冒号后开始\n",
    "                current_img = m.group(1).strip()\n",
    "                current_body_parts.append(line[m.end():])\n",
    "            else:\n",
    "                # 非 header 行：继续累积到当前条目\n",
    "                if current_img is not None:\n",
    "                    current_body_parts.append(line)\n",
    "                else:\n",
    "                    # 还没开始条目，丢弃或根据需要处理\n",
    "                    continue\n",
    "\n",
    "    # 文件结束后，写出最后一个条目\n",
    "    if current_img is not None:\n",
    "        _flush_one(current_img, \"\".join(current_body_parts), out_dir)\n",
    "\n",
    "# 调用示例\n",
    "split_streaming(\n",
    "    r\"F:\\Projects\\diffusers\\ProgramData\\new_data\\output_vertex\\output_vertex.txt\",\n",
    "    out_dir=r\"F:\\Projects\\diffusers\\ProgramData\\new_data\\image\\smpl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cameractrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
