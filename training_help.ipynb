{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 把底模safetensors转化为文件夹权重模式\n",
    "\n",
    "在这里下载底模https://civitai.com/models/27259?modelVersionId=221220然后移动到文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --content-disposition \"https://civitai.com/api/download/models/221220?type=Model&format=SafeTensor&size=pruned&fp=fp16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "checkpoint_path = r\"tmndMix_tmndMixSPRAINBOW.safetensors\"\n",
    "save_path = r\"basemodel\"\n",
    "# 加载 .safetensors 文件\n",
    "pipeline = StableDiffusionPipeline.from_single_file(checkpoint_path)\n",
    "\n",
    "# 将模型保存为 diffusers 格式\n",
    "pipeline.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 把数据集解压\n",
    "\n",
    "数据集要的所有东西全部放在共享文件夹了（images）  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip image/image_resized.zip\n",
    "!cp images/merged_joints2d.txt image_resized/merged_joints2d.txt\n",
    "!cp images/filtered_camera_params.txt image_resized/camera_params.txt\n",
    "!cp images/image_features.txt image_resized/image_features.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip images/image_mirror_resized.zip\n",
    "!cp images/mirror_merged_joints2d.txt image_resized/merged_joints2d.txt\n",
    "!cp images/mirror_filtered_camera_params.txt image_mirror_resized/camera_params.txt\n",
    "!cp images/image_features.txt image_mirror_resized/image_features.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/sodabreak/PoseCtrl.git\n",
    "!cd poseCtrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单卡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train_colab_V6_AllData.py --pretrained_model_name_or_path \"/basemodel\" --base_point_path \"/PoseCtrl/dataSet/standardVertex_2.txt\" --data_root_path_2 \"/image_resized\" --data_root_path_3 \"/image_mirror_resized\" --train_batch_size 4 --save_steps 2000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多卡\n",
    "\n",
    "- 先运行配置\n",
    "- 回答问题：使用混合精度fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate launch --num_processes 2 --multi_gpu --mixed_precision \"fp16\" train_colab_V6_AllData.py --pretrained_model_name_or_path \"/basemodel\" --base_point_path \"/PoseCtrl/dataSet/standardVertex_2.txt\" --data_root_path_2 \"/image_resized\" --data_root_path_3 \"/image_mirror_resized\" --train_batch_size 4 --save_steps 2000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 权重处理\n",
    "\n",
    "所有训练得到的权重都在/sd-pose_ctrl文件夹内\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" V6 \"\"\"\n",
    "import torch\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "current_dir = 'content'\n",
    "\n",
    "sys.path.append('/content/PoseCtrl')\n",
    "sys.path.append('/content/PoseCtrl/poseCtrl')\n",
    "from poseCtrl.models.pose_adaptor import VPmatrixPoints, ImageProjModel\n",
    "from poseCtrl.models.attention_processor import AttnProcessor, PoseAttnProcessor\n",
    "from poseCtrl.data.dataset import CombinedDataset, load_base_points\n",
    "from poseCtrl.models.posectrl import PoseCtrl,PoseCtrlV1,PoseCtrlV5\n",
    "import numpy as np\n",
    "from poseCtrl.pipelines.Pose_pipelines import PoseControlNet\n",
    "from poseCtrl.pipelines.Pose_pipelines import PoseControlNet, PoseControlNetV6\n",
    "\n",
    "base_point_path=r'/content/drive/MyDrive/PoseCtrl/dataSet/standardVertex_2.txt'\n",
    "raw_base_points=load_base_points(base_point_path)  \n",
    "base_model_path = r\"/content/drive/MyDrive/basemodel\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "ip_ckpt = r\"/content/drive/MyDrive/ckpt/points_posectrl2000.bin\"\n",
    "device = \"cuda\"\n",
    "def image_grid(imgs_np: np.ndarray, rows: int, cols: int):\n",
    "    \"\"\"\n",
    "    imgs_np: np.ndarray of shape (B, H, W, C) and values in [0, 1] (float)\n",
    "    rows, cols: number of rows and columns in grid\n",
    "    \"\"\"\n",
    "    assert imgs_np.ndim == 4 and imgs_np.shape[0] == rows * cols, \"Input shape must be (B, H, W, C)\"\n",
    "    B, H, W, C = imgs_np.shape\n",
    "\n",
    "    # 归一化 -> uint8 -> PIL\n",
    "    pil_imgs = [Image.fromarray((img * 255).astype(np.uint8)) for img in imgs_np]\n",
    "\n",
    "    # 创建大图\n",
    "    grid_img = Image.new('RGB', size=(cols * W, rows * H))\n",
    "    \n",
    "    for idx, img in enumerate(pil_imgs):\n",
    "        x = (idx % cols) * W\n",
    "        y = (idx // cols) * H\n",
    "        grid_img.paste(img, box=(x, y))\n",
    "\n",
    "    return grid_img\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "\n",
    "# load SD pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "val_dataset = CombinedDataset(\n",
    "        # path1=args.data_root_path_1,\n",
    "        path2=\"/content/drive/MyDrive/images_01/test\",\n",
    "        tokenizer=pipe.tokenizer,\n",
    "    )\n",
    "\n",
    "\n",
    "data = val_dataset[0]\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Resize((256, 256))\n",
    "\n",
    "\"\"\" denormalize 是因为dataloader的图片加载完之后会被transform，正常的图片只要是PIL就行 \"\"\"\n",
    "def denormalize(tensor, mean, std):\n",
    "    return tensor * std + mean\n",
    "\n",
    "mean = 0.5\n",
    "std = 0.5\n",
    "\n",
    "\n",
    "image = data['image']\n",
    "points = data['joints_image']\n",
    "image = denormalize(image, mean, std)\n",
    "image_pil = transforms.ToPILImage()(image) \n",
    "points_pil = transforms.ToPILImage()(points)\n",
    "points = data['joints_image'].to(torch.float16).unsqueeze(0).to(device)\n",
    "vmatrix = data['view_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "pmatrix = data['projection_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "text = 'highly detailed, anime, 1girl, blue_eyes, purple long_hair, black dress, smile, white simple background'\n",
    "pose_model = PoseControlNetV6(pipe, image_encoder_path, ip_ckpt, raw_base_points, device,8)\n",
    "images = pose_model.generate( prompt = text, num_samples=4, num_inference_steps=50, seed=42, V_matrix=vmatrix, P_matrix=pmatrix, points=points)\n",
    "grid = image_grid(images, 1, 4)\n",
    "combined_image = Image.new('RGB', (6*image_pil.width, image_pil.height))\n",
    "combined_image.paste(image_pil, (0, 0))\n",
    "combined_image.paste(points_pil, (image_pil.width, 0))\n",
    "combined_image.paste(grid, (2*image_pil.width, 0))\n",
    "combined_image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
